# 特征匹配

如何将一张图像中检测到的特征与另一张图像中的特征进行匹配？ 特征匹配涉及比较不同图像中的关键属性，以找到相似之处。特征匹配在许多计算机视觉应用中都很有用，包括场景理解、图像拼接、目标跟踪和模式识别。

## 暴力搜索

假如你有一大盒拼图碎片，并且试图找到一个可以放入你的拼图中的特定碎片。这类似于在图像中搜索匹配的特征。你没有采用任何特殊的策略，而是决定逐个检查每个碎片，直到找到合适的碎片。这种直接的方法就是暴力搜索。暴力搜索的优点是简单，你不需要任何特殊的技巧，只需要耐心。但是，它可能很耗时，尤其是在有大量碎片需要检查的情况下。在特征匹配的上下文中，这种暴力方法类似于将一张图像中的每个像素与另一张图像中的每个像素进行比较，以查看它们是否匹配。这不会有遗漏，但可能需要大量时间，尤其是对于大型图像。

现在我们对如何进行暴力匹配有了一个直观的了解，让我们深入研究算法。我们将使用在上一章中了解到的描述符来查找两张图像中匹配的特征。

首先安装并加载库。

```bash
!pip install opencv-python
```

```python
import cv2
import numpy as np
```

**使用 SIFT 的暴力搜索**

让我们从初始化 SIFT 检测器开始。

```python
sift = cv2.SIFT_create()
```

使用 SIFT 查找关键点和描述符。

```python
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)
```

使用 k 近邻查找匹配项。

```python
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1, des2, k=2)
```

应用比率测试来阈值化最佳匹配。

```python
good = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:
        good.append([m])
```

绘制匹配项。

```python
img3 = cv2.drawMatchesKnn(
    img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/SIFT.png" alt="SIFT">
</div>

**使用 ORB (二进制) 描述符的暴力搜索**

初始化 ORB 描述符。

```python
orb = cv2.ORB_create()
```

查找关键点和描述符。

```python
kp1, des1 = orb.detectAndCompute(img1, None)
kp2, des2 = orb.detectAndCompute(img2, None)
```

因为 ORB 是一个二进制描述符，所以我们使用[汉明距离](https://www.geeksforgeeks.org/hamming-distance-two-strings/)查找匹配项，
汉明距离是衡量两个等长字符串之间差异的指标。

```python
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
```

我们现在将找到匹配项。

```python
matches = bf.match(des1, des2)
```

我们可以像下面这样按距离顺序对它们进行排序。

```python
matches = sorted(matches, key=lambda x: x.distance)
```

绘制前 n 个匹配项。

```python
img3 = cv2.drawMatches(
    img1,
    kp1,
    img2,
    kp2,
    matches[:n],
    None,
    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,
)
```

**快速最近邻逼近搜索库(FLANN)**

Muja 和 Lowe 在 [Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf) 中提出了 FLANN。为了解释 FLANN，我们将继续使用我们的拼图游戏示例。想象一个巨大的拼图，数百个碎片散落各处。你的目标是根据这些碎片之间的匹配程度来组织这些碎片。FLANN 没有随机尝试匹配碎片，而是使用了一些巧妙的技巧来快速找出哪些碎片最有可能组合在一起。FLANN 不是尝试将每个碎片与所有其他碎片进行比较，而是通过查找近似相似的碎片来简化流程。这意味着它可以对哪些碎片可能很好地组合在一起进行有根据的猜测，即使它们不是完全匹配。在底层，FLANN 使用一种叫做 k-D 树的东西。可以把它想象成以一种特殊的方式组织拼图碎片。FLANN 不是将每个碎片与所有其他碎片进行比较，而是将它们排列成一种树状结构，从而加快了查找匹配项的速度。在 k-D 树的每个节点中，FLANN 将具有相似特征的碎片放在一起。这就像将具有相似形状或颜色的拼图碎片分类到不同的堆中。这样，当你在寻找匹配项时，你可以快速检查最有可能具有相似碎片的堆。假设你正在寻找一块“天空”碎片。FLANN 不会搜索所有碎片，而是会引导你到 k-D 树中的正确位置，在那里对天空颜色的碎片进行排序。FLANN 还会根据拼图碎片的特征来调整其策略。如果你有一个有很多颜色的拼图，它会专注于颜色特征。或者，如果它是一个具有复杂形状的拼图，它会注意这些形状。通过在查找匹配特征时平衡速度和准确性，FLANN 显着提高了查询时间。

首先，我们创建一个字典来指定我们将使用的算法，对于 SIFT 或 SURF，它看起来像下面这样。

```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
```

对于 ORB，将使用论文中的参数。

```python
FLANN_INDEX_LSH = 6
index_params = dict(
    algorithm=FLANN_INDEX_LSH, table_number=12, key_size=20, multi_probe_level=2
)
```

我们还创建了一个字典来指定要访问的最大叶子数，如下所示。

```python
search_params = dict(checks=50)
```

启动 SIFT 检测器。

```python
sift = cv2.SIFT_create()
```

使用 SIFT 查找关键点和描述符。

```python
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)
```

我们现在将定义 FLANN 参数。在这里，trees 是你想要的箱子的数量。

```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)
flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1, des2, k=2)
```

我们将只绘制好的匹配项，所以创建一个掩码(mask)。

```python
matchesMask = [[0, 0] for i in range(len(matches))]
```

我们可以执行比率测试来确定好的匹配项。

```python
for i, (m, n) in enumerate(matches):
    if m.distance < 0.7 * n.distance:
        matchesMask[i] = [1, 0]
```

现在让我们可视化匹配项。

```python
draw_params = dict(
    matchColor=(0, 255, 0),
    singlePointColor=(255, 0, 0),
    matchesMask=matchesMask,
    flags=cv2.DrawMatchesFlags_DEFAULT,
)

img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)
```

![FLANN](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/FLANN.png)

## 基于 Transformer 的局部特征匹配 (LoFTR)

Sun 等人在 [LoFTR：使用 Transformer 的无检测器局部特征匹配](https://arxiv.org/pdf/2104.00680.pdf) 中提出了 LoFTR。
LoFTR 没有使用特征检测器，而是使用了一种基于学习的方法来进行特征匹配。

让我们保持简单，再次使用我们的拼图示例。LoFTR 没有简单地逐像素比较图像，而是寻找每张图像中的特定关键点或特征。这就像识别每个拼图碎片的角和边。正如一个非常擅长拼图的人可能会专注于独特的标记一样，LoFTR 会识别一张图像中的这些独特点。这些可能是关键的地标或突出的结构。正如我们已经了解到的，匹配算法处理旋转或比例变化非常重要。如果某个特征被旋转或调整大小，LoFTR 仍然可以识别它。这就像解决碎片可能被翻转或调整的拼图游戏。当 LoFTR 匹配特征时，它会分配一个相似度分数，以指示特征的对齐程度。较高的分数意味着更好的匹配。这就像给一个拼图碎片与另一个拼图碎片的匹配程度打分。
LoFTR 还可以应对某些变换，这意味着它可以处理光照、角度或透视的变化。这在处理可能在不同条件下拍摄的图像时至关重要。LoFTR 稳健匹配特征的能力使其对于图像拼接等任务非常有用，在这种任务中，你可以通过识别和连接共同特征来无缝地组合多个图像。

我们可以使用 [Kornia](https://github.com/kornia/kornia) 来查找两张图像中使用 LoFTR 匹配的特征。

```bash
!pip install kornia kornia-rs kornia_moons opencv-python --upgrade
```

导入必要的库。

```python
import cv2
import kornia as K
import kornia.feature as KF
import matplotlib.pyplot as plt
import numpy as np
import torch
from kornia_moons.viz import draw_LAF_matches
```

加载并调整图像大小。

```python
from kornia.feature import LoFTR

img1 = K.io.load_image(image1.jpg, K.io.ImageLoadType.RGB32)[None, ...]
img2 = K.io.load_image(image2.jpg, K.io.ImageLoadType.RGB32)[None, ...]

img1 = K.geometry.resize(img1, (512, 512), antialias=True)
img2 = K.geometry.resize(img2, (512, 512), antialias=True)
```

指示图像是“室内”图像还是“室外”图像。

```python
matcher = LoFTR(pretrained="outdoor")
```

LoFTR 仅适用于灰度图像，因此将图像转换为灰度图像。

```python
input_dict = {
    "image0": K.color.rgb_to_grayscale(img1),
    "image1": K.color.rgb_to_grayscale(img2),
}
```

让我们执行推理。

```python
with torch.inference_mode():
    correspondences = matcher(input_dict)
```

使用随机抽样一致性算法 ([RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus)) 清理对应关系。这有助于处理数据中的噪声或异常值。

```python
mkpts0 = correspondences["keypoints0"].cpu().numpy()
mkpts1 = correspondences["keypoints1"].cpu().numpy()
Fm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)
inliers = inliers > 0
```

最后，我们可以可视化匹配项。

```python
draw_LAF_matches(
    KF.laf_from_center_scale_ori(
        torch.from_numpy(mkpts0).view(1, -1, 2),
        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),
        torch.ones(mkpts0.shape[0]).view(1, -1, 1),
    ),
    KF.laf_from_center_scale_ori(
        torch.from_numpy(mkpts1).view(1, -1, 2),
        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),
        torch.ones(mkpts1.shape[0]).view(1, -1, 1),
    ),
    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),
    K.tensor_to_image(img1),
    K.tensor_to_image(img2),
    inliers,
    draw_dict={
        "inlier_color": (0.1, 1, 0.1, 0.5),
        "tentative_color": None,
        "feature_color": (0.2, 0.2, 1, 0.5),
        "vertical": False,
    },
)
```

最佳匹配以绿色可视化，而不太确定的匹配以蓝色可视化。

![LoFTR](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/LoFTR.png)

## 资源和延伸阅读

- [FLANN Github](https://github.com/flann-lib/flann)
- [Image Matching Using SIFT, SURF, BRIEF and ORB: Performance Comparison for Distorted Images](https://arxiv.org/pdf/1710.02726.pdf)
- [ORB (Oriented FAST and Rotated BRIEF) tutorial](https://docs.opencv.org/4.x/d1/d89/tutorial_py_orb.html)
- [Kornia tutorial on Image Matching](https://kornia.github.io/tutorials/nbs/image_matching.html)
- [LoFTR Github](https://github.com/zju3dv/LoFTR)
- [OpenCV Github](https://github.com/opencv/opencv-python)
- [OpenCV Feature Matching Tutorial](https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html)
- [OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching](https://arxiv.org/abs/2204.08870)
